{"contents": "When you extract data from the web at scale, quality assurance is an important process to make sure your web extracted data is consistently of high quality. Validation of this data can be complex though. There are many challenges and problems that need to be addressed. In the second part of this series on web data quality assurance, we will cover most common hurdles and pitfalls in data validation and how to deal with them.I still remember my first lesson when I joined the team. My manager shared 3 simple questions to keep in our mind working on data validation:The problems will be listed in their natural appearance in a typical web scraping project.In the previous post, we discussed the importance of clear, testable requirements. Now let's add more details about what else could be challenging at this point.The QA department is responsible for defining good tests, both in terms of "}
{"contents": "Today we are delighted to launch a Beta of our newest data extraction API: . With this API you can collect structured data from web pages that contain automotive data such as classified or dealership sites. Using our API, you can get your data without writing site-specific code. If you need automotive/vehicle data, sign up now for a beta version of our Vehicle API.Whether you are interested in car prices, VIN or other car specific details, our Vehicle API can extract that data for you, at scale.With , you can get access to all the publicly visible details and technical information about the vehicle in a structured JSON."}
{"contents": "Web scraping projects usually involve data extraction from many websites. The standard approach to tackle this problem is to write some code to navigate and extract the data from each website. However, this approach may not scale so nicely in the long-term, requiring maintenance effort for each website; it also doesn\u2019t scale in the short-term, when we need to start the extraction process in a couple of weeks. Therefore, we need to think of different solutions to tackle these issues.The problem we propose to solve here is related to  that can be available in HTML form or files, such as PDFs. The catch is that this is required for a few hundreds of different domains and we should be able to scale it up and down without much effort.A brief outline of the problem that needs to be solved:In terms of the solution, file downloading is already built-in Scrapy, it\u2019s just a matter of finding the proper URLs to be downloaded. A routine for HTML article extraction is a bit more tricky, so for this one, we\u2019ll go with AutoExtract\u2019s "}
{"contents": "We are excited to announce our next . Using this API, you can get access to product reviews in a structured format, without writing site-specific code. You can use the Product Reviews API to extract product reviews from eCommerce sites at scale. Just make a request to the API and receive your data in real-time!In today\u2019s competitive eCommerce world, product reviews provide a great way for online shoppers to determine what products to buy. Hence, monitoring product reviews are important for businesses. Making use of reviews data, you can find insights in the data that can improve your decision making, address feedback, and monitor customer sentiment.But getting access to structured web data is not easy, especially if you don\u2019t have the right tools. With Product Reviews API, we provide a convenient way for you to extract reviews at scale from any site.More info about the fields in the "}
{"contents": "The manual way or the highway...In software testing and QA circles, the topic of whether automated or manual testing is superior remains a hotly debated one. For  QA and validation specifically, they are not mutually exclusive. Indeed, for data, manual QA can inform automated QA, and vice versa. In this post, we\u2019ll give some examples.It is rare that can be adequately validated with automated techniques alone; additional manual inspections are often needed. The optimal blend of manual and automated tests depends on factors including:"}
{"contents": "As the COVID-19 pandemic took hold, we at Scrapinghub began to wonder how it would impact on the data we crawl, and whether that data could tell us something useful about the pandemic and its impact.Retail price intelligence is one of our key areas of interest. On a daily basis,  for price and stock level information. What insights might be hidden in that data?To explore this, we identified a basket of goods related to pandemic preparedness (eg Face Masks, medications, etc) and began to track the number of individual items available online in this set, and the average item price over time.In the first chart, you can see the overall price (in blue), and the number of individual items (SKU\u2019s, in retail parlance, in red) over time for a collection of US data sources. You can see there is a lot of variation in the number of items on supply as new vendors enter the market with basic items. As you might expect, the price often moves in the opposite direction, as in the dip around April 5th. Basic economics works, but the average price doesn\u2019t drop away as strongly as you might expect. Underlying demand is clearly strong. Note that we aren\u2019t looking at stock levels here, just the range of items in stock and available for purchase. Variation in the number of items available could be due to opportunistic vendors entering the market, or rebranding routine supplies with COVID-19 related keywords."}
{"contents": "The Internet offers a in the form of , news, blog posts, stories, essays, tutorials that can be leveraged by many useful applications:But anyone interested in using all this data available, will face some challenges.\u00a0Web pages are built of many components (menus, sidebars, ads, etc) and only a few of them represent the true article content, the actual valuable information. Being able to "}
{"contents": "Scrapinghub is a fully distributed organization with a remote workforce spread across the globe. This structure will enable us to continue to operate at full capacity during the Coronavirus pandemic and deliver full service to our customers.Businesses all over the world are trying to adapt to the new circumstances brought on by the Coronavirus such as being forced to implement a remote working environment while retaining productivity, a huge challenge for companies that normally don\u2019t work remotely.We have had a lot of queries from our customers, who are doing internal global risk assessments on their supply chains being affected by COVID-19 so want to share our continued commitment to providing our customers with ongoing services during this time while ensuring a safe environment for all of our employees.Our internal risk models predict no more than 2% leave as a worst-case scenario, only slightly above baseline. This is likely to be offset by people deferring normal vacation leave due to travel limitations.Scrapinghub is at exceptionally low risk from any disruption in supplying our customers from the pandemic for two reasons:"}
{"contents": "When it comes to web scraping at scale, there\u2019s a set of challenges you need to overcome to extract the data. But once you are able to get it, you still have work to do. You need to have a data QA process in place. Data quality becomes especially crucial if you\u2019re extracting high volumes of data from the web regularly and your team\u2019s success depends on the quality of the scraped data.\n\nThis article is the first of a four-part series of how to maximize web scraped data quality. We are going to share with you all the techniques, tricks and technologies we use at Scrapinghub to extract web data from billions of pages every month, while keeping data quality high.The first step is to understand the business requirements of the web scraping project and define clear, testable rules which will help you detect data quality problems. Understanding requirements clearly is essential to move forward and develop the best data quality process."}
{"contents": "I\u2019d like to echo Joel Gasgoine\u2019s sentiments: This is not normal remote working!Like Buffer, we\u2019ve been a remote-first company for almost 10 years and we\u2019re also adjusting to the new normal as a result of COVID-19.Remote teams tend to favour asynchronous written communication. That is to say, we send text messages and don\u2019t expect an immediate response. This allows longer blocks of uninterrupted working time, it allows \u201cbatching\u201d of replies and works better when people have different working hours. Amir Salihefendic makes the case for Asynchronous Communication in his excellent blog post . This is one of the reasons why so many remote workers feel more productive.On the other hand, synchronous communication is more effective when a topic requires a lot of back-and-forths, or if there is time pressure. Don\u2019t be afraid to \u201cjump on a call\u201d to quickly sort things out. Meetings are better for brainstorming or to achieve a consensus, however, this can be more difficult remotely, especially if participants are not used to it."}
